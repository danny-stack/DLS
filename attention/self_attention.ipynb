{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4个位置,每个位置3维特征\n",
    "Feature_all = np.array([\n",
    "    [1, 0, 1],  # 位置1\n",
    "    [0, 1, 1],  # 位置2  \n",
    "    [1, 1, 0],  # 位置3\n",
    "    [0, 0, 1]   # 位置4\n",
    "])  # shape: [4, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature: [4, 3]    # 4个位置,每个位置3维特征 \\\n",
    "W_q: [3, 2]       # 3维映射到2维 \\\n",
    "Query = Feature @ W_q    # [4, 3] @ [3, 2] = [4, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取输入和输出维度\n",
    "n_in = 3   # 输入维度\n",
    "n_out = 2  # 输出维度\n",
    "\n",
    "# Xavier正态初始化\n",
    "# std = sqrt(2.0 / (n_in + n_out))\n",
    "std = np.sqrt(2.0 / (n_in + n_out))\n",
    "\n",
    "# 初始化三个权重矩阵\n",
    "W_q = np.random.normal(0, std, (n_in, n_out))\n",
    "W_k = np.random.normal(0, std, (n_in, n_out))\n",
    "W_v = np.random.normal(0, std, (n_in, n_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_q: [[ 0.1936747   0.78729825]\n",
      " [ 0.54926615 -0.04591318]\n",
      " [ 1.62722824  0.39686957]]\n",
      "W_k: [[ 0.83663886 -0.79360409]\n",
      " [-0.42752367 -0.30505612]\n",
      " [-0.68283334 -0.00779054]]\n",
      "W_v: [[-0.51232982 -0.86301211]\n",
      " [ 0.74448008 -0.28517045]\n",
      " [ 0.27987747  0.29156925]]\n",
      "Shape = (3, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"W_q: {W_q}\")\n",
    "print(f\"W_k: {W_k}\")\n",
    "print(f\"W_v: {W_v}\")\n",
    "print(f\"Shape = {W_q.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query shape: (4, 2)\n",
      "Query:\n",
      " [[1.82090294 1.18416782]\n",
      " [2.17649439 0.3509564 ]\n",
      " [0.74294086 0.74138508]\n",
      " [1.62722824 0.39686957]]\n"
     ]
    }
   ],
   "source": [
    "# 计算Query,Key,Value\n",
    "Query = Feature_all @ W_q  # [4, 2]\n",
    "Key = Feature_all @ W_k    # [4, 2]\n",
    "Value = Feature_all @ W_v  # [4, 2]\n",
    "\n",
    "print(\"Query shape:\", Query.shape)\n",
    "print(\"Query:\\n\", Query)\n",
    "# 每行表示一个位置的Query向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores:\n",
      " [[-0.66892083 -2.39231529 -0.55603902 -1.25259854]\n",
      " [ 0.05350227 -2.52648133  0.50485508 -1.48891708]\n",
      " [-0.47987362 -1.05686943 -0.5105819  -0.51308058]\n",
      " [-0.06777247 -1.93096359  0.22969898 -1.11421752]]\n",
      "Attention weights:\n",
      " [[0.35016434 0.0624901  0.39200884 0.19533672]\n",
      " [0.34964135 0.02649416 0.54908911 0.07477537]\n",
      " [0.28582211 0.16051282 0.2771784  0.27648668]\n",
      " [0.35053151 0.05439431 0.47197313 0.12310105]]\n"
     ]
    }
   ],
   "source": [
    "# 计算注意力分数\n",
    "scores = Query @ Key.T  # [4, 4]\n",
    "print(\"Attention scores:\\n\", scores)\n",
    "# scores[i,j]表示位置i对位置j的注意力分数\n",
    "\n",
    "# 归一化\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "weights = softmax(scores)\n",
    "print(\"Attention weights:\\n\", weights)\n",
    "# 每行和为1,表示位置i对所有位置的注意力权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " [[ 0.12829098 -0.59284257]\n",
      " [ 0.09426366 -0.80828286]\n",
      " [ 0.23971192 -0.3999403 ]\n",
      " [ 0.11825924 -0.7059795 ]]\n"
     ]
    }
   ],
   "source": [
    "# 最终输出\n",
    "output = weights @ Value  # [4, 2]\n",
    "print(\"Output:\\n\", output)\n",
    "# 每行是一个位置的输出特征,融合了所有位置的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position 1 attention weights: [0.35016434 0.0624901  0.39200884 0.19533672]\n",
      "Position 1 original feature: [1 0 1]\n",
      "Position 1 new feature: [ 0.12829098 -0.59284257]\n"
     ]
    }
   ],
   "source": [
    "# 位置1的权重分布\n",
    "print(\"Position 1 attention weights:\", weights[0])\n",
    "# 可以看到它对各个位置的关注程度\n",
    "\n",
    "# 位置1的最终特征是如何融合的\n",
    "print(\"Position 1 original feature:\", Feature_all[0])\n",
    "print(\"Position 1 new feature:\", output[0])\n",
    "# 可以看到新特征如何融合了全局信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "n_heads = 4  # 头数\n",
    "d_model = 3  # 输入特征维度\n",
    "d_k = d_v = 2  # 每个头的维度\n",
    "\n",
    "# 为每个头初始化权重矩阵\n",
    "# 使用Xavier初始化\n",
    "std = np.sqrt(2.0 / (d_model + d_k))\n",
    "\n",
    "# 每个头有自己的W_q, W_k, W_v\n",
    "W_q_heads = [np.random.normal(0, std, (d_model, d_k)) for _ in range(n_heads)]\n",
    "W_k_heads = [np.random.normal(0, std, (d_model, d_k)) for _ in range(n_heads)]\n",
    "W_v_heads = [np.random.normal(0, std, (d_model, d_v)) for _ in range(n_heads)]\n",
    "\n",
    "W_q_heads = np.array(W_q_heads)\n",
    "W_k_heads = np.array(W_k_heads)\n",
    "W_v_heads = np.array(W_v_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.2561395   0.17922212]\n",
      "  [ 0.34622551 -0.72346501]\n",
      "  [-0.35703322 -0.42980255]]\n",
      "\n",
      " [[-0.43398198 -0.1584815 ]\n",
      "  [-0.83777088 -0.64821551]\n",
      "  [ 0.26073526  0.07988022]]\n",
      "\n",
      " [[-0.00870527  0.69960601]\n",
      "  [ 0.42938427 -0.06887067]\n",
      "  [ 0.30984964  0.30917478]]\n",
      "\n",
      " [[ 0.50756209 -0.71378229]\n",
      "  [ 0.11222289 -0.0797183 ]\n",
      "  [ 0.02702348 -0.33147692]]]\n",
      "(4, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "print(W_q_heads)\n",
    "print(W_q_heads.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.46834058  0.46934938]\n",
      "  [ 0.31235121  0.56094363]\n",
      "  [ 0.28177941  0.48250133]\n",
      "  [ 0.53172276  0.48693824]]\n",
      "\n",
      " [[ 0.0458458  -1.08443782]\n",
      "  [ 0.03578854 -1.05753493]\n",
      "  [ 0.01530534 -1.06665533]\n",
      "  [ 0.04823435 -1.11904312]]\n",
      "\n",
      " [[ 0.92165626 -0.01353054]\n",
      "  [ 0.80494898  0.11203423]\n",
      "  [ 0.87002499  0.02854181]\n",
      "  [ 0.82970115  0.02133007]]\n",
      "\n",
      " [[ 0.90416997 -0.12297903]\n",
      "  [ 1.06334586 -0.22649126]\n",
      "  [ 0.94761556 -0.15466373]\n",
      "  [ 1.08919686 -0.24193256]]]\n",
      "(4, 4, 2)\n"
     ]
    }
   ],
   "source": [
    "def attention(Q, K, V):\n",
    "    scores = Q @ K.T\n",
    "    weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n",
    "    return weights @ V\n",
    "\n",
    "# 计算多头注意力\n",
    "head_outputs = []\n",
    "for h in range(n_heads):\n",
    "    # 每个头的注意力计算\n",
    "    Q = Feature_all @ W_q_heads[h]  # [4, 2]\n",
    "    K = Feature_all @ W_k_heads[h]  # [4, 2]\n",
    "    V = Feature_all @ W_v_heads[h]  # [4, 2]\n",
    "    \n",
    "    head_output = attention(Q, K, V)  # [4, 2]\n",
    "    head_outputs.append(head_output)\n",
    "\n",
    "head_outputs = np.array(head_outputs)\n",
    "print(head_outputs)\n",
    "print(head_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个头的输出形状: [(4, 2), (4, 2), (4, 2), (4, 2)]\n",
      "多头拼接后的形状: (4, 8)\n",
      "最终输出形状: (4, 3)\n"
     ]
    }
   ],
   "source": [
    "# 拼接多个头的输出\n",
    "multi_head_output = np.concatenate(head_outputs, axis=1)  # [4, 8]\n",
    "\n",
    "# 通常还需要一个最终的线性变换\n",
    "W_o = np.random.normal(0, std, (n_heads * d_v, d_model))\n",
    "final_output = multi_head_output @ W_o  # [4, 3]\n",
    "\n",
    "print(\"每个头的输出形状:\", [h.shape for h in head_outputs])\n",
    "print(\"多头拼接后的形状:\", multi_head_output.shape)\n",
    "print(\"最终输出形状:\", final_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "位置1在各个头的attention weights:\n",
      "Head 1: [0.26914389 0.29401485 0.19324804 0.24359321]\n",
      "Head 2: [0.22621923 0.28882462 0.21266388 0.27229227]\n",
      "Head 3: [0.31554975 0.14753084 0.26249605 0.27442336]\n",
      "Head 4: [0.12891289 0.2111176  0.54042794 0.11954157]\n"
     ]
    }
   ],
   "source": [
    "# 分析位置1的输出\n",
    "print(\"\\n位置1在各个头的attention weights:\")\n",
    "for h in range(n_heads):\n",
    "    Q = Feature_all @ W_q_heads[h]\n",
    "    K = Feature_all @ W_k_heads[h]\n",
    "    scores = Q @ K.T\n",
    "    weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n",
    "    print(f\"Head {h+1}:\", weights[0])  # 位置1的权重"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
